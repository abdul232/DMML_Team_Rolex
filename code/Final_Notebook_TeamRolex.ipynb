{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betting strategies \n",
    "# _England Premiere League Match Results & Odds Dataset_\n",
    "\n",
    "## I. Introduction\n",
    "\n",
    "> You do not know anything about sports betting or you want to implement a new strategy? This notebook will help you understand this world, and also provide you a **betting strategy** that you will be able to apply on your own.\n",
    "\n",
    "In this report, we will take the example of Football, one of the sports with the highest number of bets. More precisely, we will concentrate on **England Premiere League match results**, because its great number of matches will allow us to test and build a strategy.\n",
    "\n",
    "### Explanations on betting's vocabulary\n",
    "First, let's set up the vocabulary. While betting,  _odds_ are linked to each bet you make. Betting odds tell you how likely an event is to happen, and represents how much money you could win if your bet realizes itself.\n",
    "\n",
    "> For example, you are on a betting website. If the odd is set as 1.40 for \"Home team win\" and you put 10€ on this odd, if the Home team actually wins (so your prediction realizes itself), you will earn 4€ and get back your first 10€.\n",
    "\n",
    "There is the possibility to bet on different type of results before a match. Here, we will take into account the bets on the match result (Home team wins, Away team wins, Draw match).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Dataset & Variables\n",
    "\n",
    "The dataset we chose groups the results of the English Premier League matches **from 2008 to 2019**. It was taken from the following website:\n",
    "We also added some features to this dataset, taken from this website:\n",
    "\n",
    "The variables available are about the matches: the match's **ID**, the match's **date time**, the **Home team**, the **Away team**, the final **number of goals**, the **match result** (Home Team win, Away Team win or Draw Match), the **team's ranking** of the previous Football season (static rank at the end of the past season), the **referee** for the match, the number of **shots**, the number of **fouls**, and the other variables are not directly linked to the match statistics. These other variables are **the odds set before the match** by **different betting websites**. These odds are set on the **final result** of a match (Home Team win, Away Team win, Draw Match).\n",
    "\n",
    "## III. Goal of the project & models\n",
    "\n",
    "The goal of the project is to build a model that can predict the following output : Home Team win, Away Team win or Draw match.\n",
    "Of course, the model will not predict perfectly the output, but by knowing which match result have the most chances to happen can create some winning bets. This will be done through **Classifications** models.\n",
    "\n",
    "We decided to build two models, taking into acount different independant variables: \n",
    "- One time, we will try to predict a match result based on the **betting websites' predictions**, that can be seen **through the odds** websites put on the possible result. \n",
    "\n",
    "- Another time we will use the **Match Statistics**, with the rank of each team, the number of shots/fouls, the number of goals, etc. \n",
    "\n",
    "In order to obtain these predictions, we will also try two classifications models: **Descision Trees** and **Logistic Regressions**. \n",
    "\n",
    "> In the end, we will evaluate the relevance of each classification model, and also define **which independant variables** (Odds or Statitics of the Match) **explained the best the Match Result**.\n",
    "\n",
    "## IV. Methodology\n",
    "\n",
    "1. At first, we will import the packages that we will need during this analyse.\n",
    "2. Then, we will upload, clean and add the necessary variables to complete the Dataset.\n",
    "3. Some variables will be transformed the variables in a way we can use them properly.\n",
    "4. Following this point, we will create scatter plots to show the repartition between the realized prediction vs non realized prediction. (When the match Odd is set on Home win => 0 the match result is Home did not win, 1 when Home won) \n",
    "5. Consequently, we will create the classification models (Logistic regression, then decision trees). It will be done for each match result, using one time the match statistics, the other time using the odds. \n",
    "6. We will also evaluate the accuracy of the models and create confusion matrixes to have a visual representation of the project.\n",
    "7. Finally, we will draw some conclusions that we can be retrieved from these classfications and methodologies.\n",
    "\n",
    "> We will also analyse our models and try to tink about how we could improve them and how they could be useful for business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The dataset\n",
    "\n",
    "The following table shows the 10 first rows of the dataset. It was uploaded from a csv file, and we decided yo name it **_data_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1574259812216,
     "user": {
      "displayName": "Abdoo Aloraib",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCUO9oHDV-z7IuSjn4Tf1zgFORfDmiFylWCNgAY=s64",
      "userId": "10713939061096290051"
     },
     "user_tz": -60
    },
    "id": "imYCkCGmMNIw",
    "outputId": "a9c39db3-798c-4d3d-d073-a62a7c8b71d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We use the Dataset with the games of the season 2008 to 2018\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/abdul232/DMML_Team_Rolex/master/data/England_2008_2018_Premiere_League_Final.csv',sep=\";\")\n",
    "\n",
    "# view of the first rows \n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have some _NaN_ variables in our data due to a format change, so we have to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop([\"Unnamed: 49\", \"Unnamed: 50\", \"Unnamed: 51\",\"Unnamed: 52\",\"Unnamed: 53\",\"Unnamed: 54\",\"Unnamed: 55\",\"Unnamed: 56\",\"Unnamed: 57\",\"Unnamed: 58\",\"Unnamed: 59\",\"Unnamed: 60\",\"Unnamed: 61\",\"Unnamed: 62\",\"Unnamed: 63\",\"Unnamed: 64\",\"Unnamed: 65\",\"Unnamed: 66\",\"Unnamed: 67\",\"Unnamed: 68\",\"Unnamed: 69\"], axis=1)\n",
    "\n",
    "#We have a new dimension\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have also seen that some rows were showing _NaN_ data due to missing odds, so we removed them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this dataset (without the _NaN_ ) counts **4'176** rows for **49** columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"Home Team Goals\",\"Away Team Goals\",\"Betbrain Numbers of bookmakers 1X2\",\"Betbrain Average Home\",\"Betbrain Maximum Draw\",\"Betbrain Average Draw\",\"Betbrain Maximum Away\",\"Betbrain Average Away\",\"Betbrain Numbers of bookmakers Goals\",\"Betbrain Max > 2.5 Goals\",\"Betbrain Average > 2.5 Goals\",\"Betbrain Max < 2.5 Goals\",\"Betbrain Average < 2.5 Goals\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Changing variables types\n",
    "\n",
    "Now, as we settled up the **types of the variables**. We will change some as int (integer), the date as datetime, while other will remain objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We have to change the type of some variable (integer)\n",
    "data['Match_ID'] = data.Match_ID.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We have to change the type of some variable (integer)\n",
    "data[['Home Team Shots','Away Team Shots', 'Home Team Shots on Target', 'Away Team Shots on Target', 'Home Fouls Committed', 'Away Fouls Committed', 'Home Corners', 'Away Corners', 'Home Yellow Cards', 'Away Yellow Cards', 'Home Red Cards', 'Away Red Cards']]= data[['Home Team Shots','Away Team Shots', 'Home Team Shots on Target', 'Away Team Shots on Target', 'Home Fouls Committed', 'Away Fouls Committed', 'Home Corners', 'Away Corners', 'Home Yellow Cards', 'Away Yellow Cards', 'Home Red Cards', 'Away Red Cards']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We have to change the type of some variable (Date)\n",
    "data['Date'] = pd.to_datetime(data['Date'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Tendencies through scatter plots\n",
    "\n",
    "A match has 3 different possible results (the Home Team wins, the Away Team wins or a Draw match). Each of these results are likely to happen, or not happen. This fact shows **the need of a binary variable**, for each of the match result (Home, Away or Draw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We wo, one for the Home team wins, one for the draws and one for the Away team wins\n",
    "data = pd.get_dummies(data, columns=['Match Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a betting strategy, it is first relevant to know how frequently the betting companies makes right predicting or not. So, in order to understand this fact, we will build **three different scatter plot**, showing the realised and not realised predictions :\n",
    "\n",
    "- When the official result is the victory of the home team crossed with the \"Home team win\" odd.\n",
    "- When the official result is the victory of the away team crossed with the \"Away team win\" odd.\n",
    "- When the official result is draw match crossed with the \"Draw\"odd.\n",
    "\n",
    "We will build scatter plots for each of the 3 differents possible results we have in a match, and compare them with the odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home team wins scenario: if the Home team wins the output is 1,  otherwise it's 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of Homewin vs No Home win\n",
    "Homewin = data[\"Match Result_H\"].value_counts()[1]\n",
    "NoHomewin = data[\"Match Result_H\"].value_counts()[0]\n",
    "print(\"Home won:\", Homewin)\n",
    "print(\"Home did not win:\", NoHomewin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common class is the Home Team did not win so we use it for calculate the **base rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base rate \n",
    "# the base rate of the No Home Win\n",
    "BaseRate = NoHomewin/data['Match Result_H'].count()\n",
    "print(\"Base Rate:\", BaseRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "We can see that we have an almost 50-50 repartition between the Match results Home win vs No Home win. 53% of the time the Home team does not win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "#tips = sns.load_dataset(data)\n",
    "\n",
    "a_B365_Home = sns.scatterplot(x=\"B365 Home\", y=\"Match Result_H\", data=data)\n",
    "\n",
    "a_BetWin_Home = sns.scatterplot(x=\"Bet&Win Home\", y=\"Match Result_H\", data=data)\n",
    "\n",
    "a_Interwetten_Home = sns.scatterplot(x=\"Interwetten Home\", y=\"Match Result_H\", data=data)\n",
    "\n",
    "a_WilliamHill_Home = sns.scatterplot(x=\"William Hill Home\", y=\"Match Result_H\", data=data)\n",
    "\n",
    "a_VCBet_Home = sns.scatterplot(x=\"VC Bet Home\", y=\"Match Result_H\", data=data)\n",
    "\n",
    "a_VCBet_Home.set(xlabel='Website Odd while Home Team wins', ylabel='Home team won [1]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the higher the odd is, the less a team has chance to win. However, it still happens sometimes, like the 5 cases where people won more than 13 times their first bet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw match scenario: if the result is draw, the output is 1,  otherwise it's 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Draw vs No Draw\n",
    "Draw = data[\"Match Result_D\"].value_counts()[1]\n",
    "NoDraw = data[\"Match Result_D\"].value_counts()[0]\n",
    "print(\"Draw match:\", Draw)\n",
    "print(\"No Draw match:\", NoDraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base rate \n",
    "# the base rate of the No Draw\n",
    "BaseRateD = NoDraw/data['Match Result_D'].count()\n",
    "print(\"Base Rate\", BaseRateD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "We can see that we have an almost 75-25 repartition. So 75% of the matches does not end as draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_B365_Draw = sns.scatterplot(x=\"B365 Draw\", y=\"Match Result_D\", data=data)\n",
    "\n",
    "a_BetWin_Draw = sns.scatterplot(x=\"Bet&Win Draw\", y=\"Match Result_D\", data=data)\n",
    "\n",
    "a_Interwetten_Draw = sns.scatterplot(x=\"Interwetten Draw\", y=\"Match Result_D\", data=data)\n",
    "\n",
    "a_WilliamHill_Draw = sns.scatterplot(x=\"William Hill Draw\", y=\"Match Result_D\", data=data)\n",
    "\n",
    "a_VCBet_Draw = sns.scatterplot(x=\"VC Bet Draw\", y=\"Match Result_D\", data=data)\n",
    "\n",
    "a_VCBet_Draw.set(xlabel='Website Odd while Draw match', ylabel='Draw [1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the same trend as the previous graph for the small odds. However, the high odds just never realize themselves, it stays below 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Away team wins scenario: if the Away team wins the output is 1,  otherwise it's 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Away win vs No Away win\n",
    "Awaywin = data[\"Match Result_A\"].value_counts()[1]\n",
    "NoAwaywin = data[\"Match Result_A\"].value_counts()[0]\n",
    "print(\"Away won:\", Awaywin)\n",
    "print(\"Away did not win:\", NoAwaywin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base rate \n",
    "# the base rate of the No Home Win\n",
    "BaseRateA = NoAwaywin/data['Match Result_A'].count()\n",
    "print(\"Base Rate\", BaseRateA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "We can see that we have a almost 70-30 repartition. So 71% of the time, the away team does not win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_B365_Away = sns.scatterplot(x=\"B365 Away\", y=\"Match Result_A\", data=data)\n",
    "\n",
    "a_BetWin_Away = sns.scatterplot(x=\"Bet&Win Away\", y=\"Match Result_A\", data=data)\n",
    "\n",
    "a_Interwetten_Away = sns.scatterplot(x=\"Interwetten Away\", y=\"Match Result_A\", data=data)\n",
    "\n",
    "a_WilliamHill_Away = sns.scatterplot(x=\"William Hill Away\", y=\"Match Result_A\", data=data)\n",
    "\n",
    "a_VCBet_Away = sns.scatterplot(x=\"VC Bet Away\", y=\"Match Result_H\", data=data)\n",
    "\n",
    "a_VCBet_Away.set(xlabel='Website Odd while Away Team wins', ylabel='Away team won [1]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the same trend as the previous graph for the small odds. However, high odds do realizes themselves and more frequently than for Home win. While for home win we only have a few (<10) between [10;20], for Away win there are a lot more of odds realizing themselves (>15) between [10;50].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 1 - BaseRate + 1- BaseRateD + 1 - BaseRateA\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sum all the percentage of _1 - Base rate_ we obtain 0,999, so the distribution tends to 1 and is almost perfect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison between two Classification models using the odds\n",
    "\n",
    "We will compare two model:\n",
    "\n",
    "> - **Logistic Regressions**: comparing the **Odd prediction** to the **Statistics of the match prediction**\n",
    "> - **Decision Trees**: comparing the **Odd prediction** to the **Statistics of the match prediction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the models, we will **normalise** the odds numbers to create a common scale for all the odds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_norm = ['B365 Home','B365 Draw','B365 Away','Bet&Win Home','Bet&Win Draw','Bet&Win Away','Interwetten Home','Interwetten Draw','Interwetten Away','William Hill Home','William Hill Draw','William Hill Away','VC Bet Home','VC Bet Draw','VC Bet Away']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])\n",
    "\n",
    "\n",
    "data[cols_to_norm].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. With the odds\n",
    "\n",
    ">We will first build 3 models based on the odds, one for each possible game results (Home win, Away win, Draw), using a **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is **the 1st logistic regression for Home Team Win:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['B365 Home','Bet&Win Home','Interwetten Home','William Hill Home','VC Bet Home']\n",
    "\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_H\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression with 5 fold cross validation\n",
    "LR = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best regulariser parameter\n",
    "LR.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set accuracy is about 64%. So this model predict correclty 64% of the cases where the output is a Home win or not. We had a default rate of almost 46% for the Home win, which is not a very good prediction but it still allows to predict something significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy is about 60%. Unfortunately, the prediction based on the test set are less good than the train set. We could optimize it by doing a Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, LR.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "We can already see the repartition of results and predictions, but just below the confusion matrix shows it in a more readable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix, (code from the Lab 5.0)\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_pred = LR.predict(X_train)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"No Home Win\", \"Home Win\"])\n",
    "    ax.yaxis.set_ticklabels([\"No Home Win\", \"Home Win\"])\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_train, y_pred, classes = y[unique_labels(y_train, y_pred)],title='Confusion matrix, with normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "> We can see we have a majority of True Negative (Prediction = No Home win and reality No Home Win) and True Positive (Prediction Home win and reality Home win), which is coherent with the result we found above. The matrix allows us to have a better visual understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    "> We have a precision of 62.06%. So, over all the predicted _Home wins_ (Predicted Positives), we have 62% of them that were really _Home wins_ (True Positives).\n",
    "Over all the actual Home wins (Actual Positives), only 60.04% of the _Home wins_ have been correctly identified (True Positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Here is the 2nd logistic regression for Draws:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['B365 Draw','Bet&Win Draw','Interwetten Draw','William Hill Draw','VC Bet Draw']\n",
    "\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression with 5 fold cross validation\n",
    "LRD = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRD.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best regulariser parameter\n",
    "LRD.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Logistic Regression cannot provide a result, which is probably due to the data distribution between the train and test set. We could create a sample to maybe resolve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "LRD.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set accuracy is about 75.48%, the model predict correclty 75.48% if it was a Draw or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "LRD.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost the same as for the train accuracy. If we compare the test accuracy (73,9%) to the base rate (71%), it is not a relevant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, LRD.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix, (code from the Lab 5.0)\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_pred = LRD.predict(X_train)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"No Draw\", \"Draw\"])\n",
    "    ax.yaxis.set_ticklabels([\"No Draw\", \"Draw\"])\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_train, y_pred, classes = y[unique_labels(y_train, y_pred)],title='Confusion matrix, with normalization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "> We can see that this confusion matrix shows no True Positive and no False Positive. This is due to the fact that the proportion of Draw are too small comparing to the total number of games, and the Train/Test split with the Logistic regression model cannot show any result. So, we decided to drop this Logistric regression model for the draws matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the 3rd logistic regression for Away wins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['B365 Away','Bet&Win Away','Interwetten Away','William Hill Away','VC Bet Away']\n",
    "\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression with 5 fold cross validation\n",
    "LRA = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRA.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best regulariser parameter\n",
    "LRA.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "LRA.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "LRA.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Same observation as for the draw, our accuracy rate of the training and the test set are to close to the base rate so we cannot use this model to predict an Away win or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, LRA.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix, (code from the Lab 5.0)\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_pred = LRA.predict(X_train)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"No Away win\", \"Away win\"])\n",
    "    ax.yaxis.set_ticklabels([\"No Away win\", \"Away win\"])\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_train, y_pred, classes = y[unique_labels(y_train, y_pred)],title='Confusion matrix, with normalization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "> As for the draws, we do not have True Positive anf False Positive, again due to the distribution of Away wins versus no Away wins. As for Draws, we will not analyse further the Logistc Regression for the Away wins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### II. With the Match Statistics\n",
    "\n",
    ">We will now build 3 models based on the games' statistics, one for each possible game results (Home win, Away win, Draw), using a **Logistic Regression**. \n",
    "\n",
    "__Note:__ We did not know how to use the previous game statistics (the match before the actual one) to do predictions about the result of a game, because we did not know how to use Time Series. So we used the statisics of the actual game to validate the relevance of games'statistics for predictions.\n",
    "\n",
    "**Logistic Regression for Home win with statistics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Home ex-Rank\",\"Home Team Shots\",\"Away ex-Rank\", \"Away Team Shots\",\"Home Team Shots on Target\", \"Away Team Shots on Target\", \"Home Fouls Committed\", \"Away Fouls Committed\", \"Home Corners\", \"Away Corners\", \"Home Yellow Cards\", \"Away Yellow Cards\", \"Home Red Cards\", \"Away Red Cards\"]\n",
    "#feature_names = ['B365 Home']\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_H\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression with 5 fold cross validation\n",
    "LR_MS = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_MS.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best regulariser parameter\n",
    "LR_MS.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "LR_MS.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "LR_MS.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This equation with match's statistics gave a better accuracy for the training set and the test set than with the Odds prediction for the case of Home win. So, 71% of the predictions were correct and realized themselves.\n",
    "\n",
    "In comparison to the Default rate (almost 46%) of an Home win, this model shows better the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, LR_MS.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix, (code from the Lab 5.0)\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_pred = LR_MS.predict(X_train)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"No Home win\", \"Home win\"])\n",
    "    ax.yaxis.set_ticklabels([\"No Home win\", \"Home win\"])\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_train, y_pred, classes = y[unique_labels(y_train, y_pred)],title='Confusion matrix, with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "We can see that we have a majority of True Positive and True Negative labels. There are a smaller number of False Positive and False Negative with the Logistic Regression using Match Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 70.91%, so over all the predicted Home wins (Predicted Positives) we have 70.91% of them that actually were Home wins (True Positives).\n",
    "> Over all the actual Home wins (Actual Positives), only 65.81% have been correctly identified (True Positives).\n",
    "\n",
    "In comparaison to the model using odds, this model shows a better precision and recall using match statistics, and then is more relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression for Draws with statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Home ex-Rank\",\"Home Team Shots\",\"Away ex-Rank\", \"Away Team Shots\",\"Home Team Shots on Target\", \"Away Team Shots on Target\", \"Home Fouls Committed\", \"Away Fouls Committed\", \"Home Corners\", \"Away Corners\", \"Home Yellow Cards\", \"Away Yellow Cards\"]\n",
    "#feature_names = ['B365 Home']\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression with 5 fold cross validation\n",
    "LR_MSD = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_MSD.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best regulariser parameter\n",
    "LR_MSD.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "LR_MSD.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "LR_MSD.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "Here we face the same problem that we had with the odds, the training set and test set are to close to the base rate. Let's have a look at the confusion matrix to have additionnal informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, LR_MSD.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix, (code from the Lab 5.0)\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_pred = LR_MSD.predict(X_train)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"No Draw\", \"Draw\"])\n",
    "    ax.yaxis.set_ticklabels([\"No Draw\", \"Draw\"])\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_train, y_pred, classes = y[unique_labels(y_train, y_pred)],title='Confusion matrix, with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Due to the distribution of draws and no draws between the train/test split, the model is not able to predict True Positive and False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression for Away win with statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Home ex-Rank\",\"Home Team Shots\",\"Away ex-Rank\", \"Away Team Shots\",\"Home Team Shots on Target\", \"Away Team Shots on Target\", \"Home Fouls Committed\", \"Away Fouls Committed\", \"Home Corners\", \"Away Corners\", \"Home Yellow Cards\", \"Away Yellow Cards\", \"Home Red Cards\", \"Away Red Cards\"]\n",
    "#feature_names = ['B365 Home']\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression with 5 fold cross validation\n",
    "LR_MSA = LogisticRegressionCV(solver='lbfgs', cv=5, max_iter=1000, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_MSA.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best regulariser parameter\n",
    "LR_MSA.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "LR_MSA.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "LR_MSA.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "The accuracy of the training set and the test set are very close to the base rate so let's see the confusion matrix to have a better understanding of the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, LR_MSA.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix, (code from the Lab 5.0)\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_pred = LR_MSA.predict(X_train)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    \n",
    "    ax.xaxis.set_ticklabels([\"No Away win\", \"Away win\"])\n",
    "    ax.yaxis.set_ticklabels([\"No Away win\", \"Away win\"])\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_train, y_pred, classes = y[unique_labels(y_train, y_pred)],title='Confusion matrix, with normalization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Here we have an interesting distribution: in contrary to the model with the odds for Away win, this time the confusion matrix shows some results, even if the distribution between train/test split is not much relevant. Here we have True Positives and False Positives, our model can predict if a game is an Away win or not. However, it is still not much precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 68.46%, so over all the predicted Away wins (Predicted Positives) we have 70.91% of them that actually were Away wins (True Positives).\n",
    ">Over all the actual Away wins (Actual Positives), only 42.41% of the Away wins have been correctly identified (True Positives).\n",
    "\n",
    "In comparison to the model using odds, even if the recall is low, we still have a precision and recall using the match statistics. This model is way more relevant than the one with the odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Decision Trees\n",
    "\n",
    "### I. With the Odds\n",
    "\n",
    "**Decision Tree for Home win**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['B365 Home','Bet&Win Home','Interwetten Home','William Hill Home','VC Bet Home']\n",
    "\n",
    "X = data[feature_names]\n",
    "y = data[\"Match Result_H\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "accuracy = clf.score(X_test,y_test)\n",
    "print('The accuracy is ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the decision tree\n",
    "Depth = clf.get_depth()\n",
    "Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This accuracy is very close to the Base rate. The Depth is also too large, so there is probably an overfitting problem with the model. We have to find the optimal depth to optimize this Decision Tree model for Home wins with the Odds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = DecisionTreeClassifier(criterion='entropy')\n",
    "scores = [clf.score(X_test,y_test)]\n",
    "for d in range(1, Depth):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores,\"r\")\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('depth', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Depth = np.argmax(scores)\n",
    "print(\"The optimal Depth is\", Max_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph above shows that the Depth maximizing the best the accuracy is **1**. So it means the best Decision Tree is when the iteration stops after the 1st split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMax = DecisionTreeClassifier(criterion='entropy', max_depth=Max_Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMax.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMax.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a better accuracy with the optimization of the Depth. However it is still is lower than the accuracy found with the Logistic Regression (Odds for Home win)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 10.57%, so over all the predicted Home wins (Predicted Positives) we have 10.57% of them that actually were Home wins (True Positives).\n",
    ">Over all the actual Home wins (Actual positives), only 4.04% of the Home wins have been correctly identified (True Positives).\n",
    "\n",
    "In comparison to the Logistic Regression model, the decision tree is less relevant and it does not allow us to predict the Home wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree for Draw**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_namesD = ['B365 Draw','Bet&Win Draw','Interwetten Draw','William Hill Draw','VC Bet Draw']\n",
    "\n",
    "X = data[feature_names]\n",
    "y = data[\"Match Result_D\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfD = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "accuracy_D = clfD.score(X_test,y_test)\n",
    "print('The accuracy is ', accuracy_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the decision tree\n",
    "Depth = clfD.get_depth()\n",
    "Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This accuracy is below the Base rate. The Depth is also too large, so there is probably an overfitting problem with the model. We have to find the optimal depth to optimize this Decision Tree model for Draws with the Odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [clfD.score(X_test,y_test)]\n",
    "for d in range(1, Depth):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores,\"g\")\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('depth', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Depth = np.argmax(scores)\n",
    "print(\"The optimal Maximum Depth is\", Max_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph above shows again that the Depth maximizing the best the accuracy is **1**. So it means the best Decision Tree is when the iteration stops after the 1st split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxD = DecisionTreeClassifier(criterion='entropy', max_depth=Max_Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxD.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 20.97%, so over all the predicted Draws (Predicted Positives) we have 20.97% of them that actually were Draws (True Positives).\n",
    ">Over all the actual Draw (Actual Positives), only 15.26% of the Home wins have been correctly identified (true Positives).\n",
    "\n",
    "This Decision Tree model is not relevant. However, we still have some results, while with the Logistic Regression model with odds has no result at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree for Away win**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_namesA = ['B365 Away','Bet&Win Away','Interwetten Away','William Hill Away','VC Bet Away']\n",
    "\n",
    "X = data[feature_names]\n",
    "y = data[\"Match Result_A\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfA = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfA.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "accuracy_A = clfA.score(X_test,y_test)\n",
    "print('The accuracy is ', accuracy_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# depth of the decision tree\n",
    "Depth = clfA.get_depth()\n",
    "Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This accuracy is below the Base rate. The Depth is also too large, so there is probably an overfitting problem with the model. We have to find the optimal depth to optimize this Decision Tree model for Away wins with the Odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = [clfA.score(X_test,y_test)]\n",
    "for d in range(1, Depth):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('depth', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Depth = np.argmax(scores)\n",
    "print(\"The optimal Maximum Depth is\", Max_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph above shows that the Depth maximizing the best the accuracy is **2**. So it means the best Decision Tree is when the iteration stops after the 2nd split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxA = DecisionTreeClassifier(criterion='entropy', max_depth=Max_Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxA.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxA.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 68.46%, so over all the predicted Away wins (Predicted Positives) we have 68.46% of them that actually were Away wins (True Positives).\n",
    ">Over all Actual Away wins (Actual Positives), only 42.41% of the Away wins have been correctly identified (True Positives).\n",
    "\n",
    "The Decision Tree model with Odds predict better the Away wins compared to the Logistic Regression model with Odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. With the Match Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match Statistics for Home win**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Home ex-Rank', 'Home Team Shots', 'Away ex-Rank', 'Away Team Shots', 'Home Team Shots on Target', 'Away Team Shots on Target', 'Home Fouls Committed', 'Away Fouls Committed', 'Home Corners', 'Away Corners', 'Home Yellow Cards', 'Away Yellow Cards', 'Home Red Cards', 'Away Red Cards']\n",
    "\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_H\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfHS = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfHS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "clfHS.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the decision tree\n",
    "clfHS.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This accuracy is above the Base rate. The Depth is also too large, so there is probably an overfitting problem with the model. We have to find the optimal depth to optimize this Decision Tree model for Home wins with the Match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [clfHS.score(X_test,y_test)]\n",
    "for d in range(1, Depth):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores, \"r\")\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('depth', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Depth = np.argmax(scores)\n",
    "print(\"The optimal Maximum Depth is\", Max_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph above shows that the Depth maximizing the best the accuracy is **5**. So it means the best Decision Tree is when the iteration stops after the 5th split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxHS = DecisionTreeClassifier(criterion='entropy', max_depth=Max_Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxHS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxHS.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 10.57%, so over all the predicted Home wins (Predicted Positives) we have 68.46% of them that actually were Home wins (True Positives).\n",
    ">Over all the actual Home wins (Actual Positives), only 4.04% of the Home wins have been correctly identified (True Positives).\n",
    "\n",
    "The Decision Tree model with Match statistics does not predict well the Home wins, compared to the Logistic Regression model with Match statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match Statistics for Draw**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Home ex-Rank', 'Home Team Shots', 'Away ex-Rank', 'Away Team Shots', 'Home Team Shots on Target', 'Away Team Shots on Target', 'Home Fouls Committed', 'Away Fouls Committed', 'Home Corners', 'Away Corners', 'Home Yellow Cards', 'Away Yellow Cards', 'Home Red Cards', 'Away Red Cards']\n",
    "\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfDS = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfDS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "clfDS.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the decision tree\n",
    "clfDS.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This accuracy is below the Base rate. The Depth is also too large, so there is probably an overfitting problem with the model. We have to find the optimal depth to optimize this Decision Tree model for Draws with the Match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [clfDS.score(X_test,y_test)]\n",
    "for d in range(1, Depth):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores,\"g\")\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('depth', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Depth = np.argmax(scores)\n",
    "print(\"The optimal Maximum Depth is\", Max_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph above shows that the Depth maximizing the best the accuracy is **4**. So it means the best Decision Tree is when the iteration stops after the 4th split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxDS = DecisionTreeClassifier(criterion='entropy', max_depth=Max_Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxDS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxDS.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 20.97%, so over all the predicted Draw (Predicted Positives) we have 20.97% of them that actually were Draws (True positives).\n",
    ">Over all the actual Draws (Actual Positives), only 15.26% of the Draws have been correctly identified (True positives).\n",
    "\n",
    "The Decision Tree model with Match statistics does not predict well the Draws, but it shows for the first time a recall and precision for the Draws, while we had no results at all with the Logistic Regression Model with Match statistics or Odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match Statistics for Away**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Home ex-Rank', 'Home Team Shots', 'Away ex-Rank', 'Away Team Shots', 'Home Team Shots on Target', 'Away Team Shots on Target', 'Home Fouls Committed', 'Away Fouls Committed', 'Home Corners', 'Away Corners', 'Home Yellow Cards', 'Away Yellow Cards', 'Home Red Cards', 'Away Red Cards']\n",
    "\n",
    "X = np.array(data[feature_names])\n",
    "y = np.array(data[\"Match Result_A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfAS = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfAS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "clfAS.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the decision tree\n",
    "clfAS.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "This accuracy is below the Base rate. The Depth is also too large, so there is probably an overfitting problem with the model. We have to find the optimal depth to optimize this Decision Tree model for Away wins with the Match statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [clfAS.score(X_test,y_test)]\n",
    "for d in range(1, Depth):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=d)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('depth', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Max_Depth = np.argmax(scores)\n",
    "print(\"The optimal Maximum Depth is\", Max_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph above shows that the Depth maximizing the best the accuracy is **4**. So it means the best Decision Tree is when the iteration stops after the 4th split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxAS = DecisionTreeClassifier(criterion='entropy', max_depth=Max_Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxAS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfMaxAS.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "Precision = precision_score(y_train, y_pred)\n",
    "\n",
    "print(Precision)\n",
    "\n",
    "\n",
    "Recall = recall_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the precision recall:\n",
    "\n",
    ">We have a precision of 68.46%, so over all the predicted Away wins (Predicted Positives) we have 68.46% of them that actually were Away wins (True Positives).\n",
    ">Over all the atual Away wins (Actual positives), only 42.41% of the Away wins have been correctly identified (True Positives).\n",
    "\n",
    "The Decision Tree model with Match statistics does still not predict well the Away wins. However, it is the same results that with the Logistic Regression Model with Match statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this Data analysis, we figured out that it can be very complex to predict a match result, and to build a betting strategy on it. So, this Notebook will mostly gives insights and trends on where to bet rather than establishing a strong betting strategy. \n",
    "\n",
    "While comparing the two different classification models (Logistic Regression vs Decision Trees) with two different equations (Odds vs Match statistics), we discovered the following interesting points:\n",
    "\n",
    "> Firstly, we observed that the predictions using the games’ statistics are significantly better than the ones with the odds. It is also important to keep in mind that we used the statistics of an already played game, and we did not use Time Series. \n",
    "\n",
    "> Secondly, using two different equations helped us to understand that odds does not perfectly represent a prediction of a match. Effectively, Betting websites are a lot playing with the odds' numbers in order to push people to bet on specific match's results. They do not only take account of the statistics of the match, but they also use the players'emotions to incite the people to play.\n",
    "\n",
    "> Thirdly, we saw that the Logistic Regression shows most of the time a better accuracy rate than the Decision Tree method. However, it is better to use Decision Trees when we have an inequality in the distribution of the outputs (match's result). Even if the precision and recall are not good, we are still able to calculate them with the Decision Trees while it is not possible with the Logistic Regression (no results found).\n",
    "\n",
    "To conclude, we observed significant results doing this analysis. We understood which Classification model is better to use for each different data's distribution. We also figured out that, people who wants to bet, should use the match statistics rather than odds to do their predictions.\n",
    "\n",
    "Nowadays, there is a trend to create websites giving predictions on sports results. So, a company could use this notebook to build a website selling forecasts of match.\n",
    "\n",
    "**Retrospective on the project**\n",
    "\n",
    "With more time and deeper knowledge of data analytics with Python, we could have improved this model. For example, by adding other variables like the actual market value of each team, we could have better this model. Even by adding a real time ranking of each team, it would surely precise the model. Finally, we could have used time series to use the previous statistics of a match, or also add some conditions to select some specific bets."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
